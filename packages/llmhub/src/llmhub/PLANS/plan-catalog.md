# PLAN-catalog-v1.md

## 0. Context and Intent

This plan defines the **Catalog** feature inside the `llm-hub` project.

**Goal (in one sentence)**  
Given the models I can actually call via `any-llm`, and external metadata sources (`models.dev` and LMArena’s `arena-catalog`), build a **local canonical catalog** of models with cost/quality/capability information, cache it, and expose it to the generator and CLI.

### Inputs

At runtime, Catalog consumes:

1. **Local environment + any-llm configuration**
   - API keys for providers (OpenAI, Anthropic, Gemini, DeepSeek, Qwen, etc.).
   - any-llm’s provider/model configuration.
   - Logical result: list of `(provider, model_id)` that are **callable** on this machine.

2. **External metadata sources**
   - `https://models.dev/api.json`  
     → authoritative, maintained model metadata: pricing, limits, modalities, flags, dates.
   - **arena-catalog** JSON generated by `lmarena/arena-catalog/update_leaderboard_data.py`  
     → per-model Arena scores (Elo-based quality signals).

3. **Static mapping overrides**
   - A small `overrides.json` committed in `llmhub/catalog/` to resolve naming / ID mismatches between sources.

### Outputs

Catalog produces:

1. A **canonical in-memory object**:

   ```python
   Catalog {
     catalog_version: int
     built_at: str
     models: list[CanonicalModel]
   }

where each CanonicalModel represents one model that:
	•	you can actually call (present in any-llm), and
	•	is enriched with as much metadata and scoring as possible.

	2.	A cached JSON file:
	•	Path: ~/.config/llmhub/catalog.json (or OS-appropriate config dir).
	•	Contains serialized Catalog.
	•	Used to avoid hitting external endpoints on every CLI/generator invocation.
	3.	A stable Python API:
	•	llmhub.catalog.build_catalog(...) -> Catalog
	•	llmhub.catalog.load_cached_catalog(...) -> Catalog | None
	4.	CLI commands:
	•	llmhub catalog show  → human inspection of current catalog.
	•	llmhub catalog refresh  → force a rebuild and cache update.

Logical responsibilities

Catalog is responsible for:
	•	Discovering which models are available via any-llm for the current user.
	•	Fetching structured metadata from models.dev and quality scores from LMArena.
	•	Fusing these sources using a canonical model ID and a small override mapping.
	•	Deriving business-friendly tiers (cost/quality/reasoning/creative) from real data.
	•	Producing a canonical model representation that the generator and UI can use for:
	•	Filtering (reasoning vs non-reasoning, tools vs no-tools, etc.).
	•	Ranking (cost vs quality vs latency preferences).
	•	Caching the result with a Time-To-Live (TTL) for performance and robustness.

Catalog is not responsible for:
	•	Calling LLMs for “research”.
	•	Deciding which model to use for a given role (that is generator’s job).
	•	Representing project-specific configs; it is a global view of available models.

⸻

1. Directory and Module Layout

All code for Catalog lives inside packages/llmhub/src/llmhub/catalog/.

Proposed layout:

llmhub/catalog/
  __init__.py
  schema.py            # Pydantic models and canonical structures
  anyllm_source.py     # Discover models from any-llm
  modelsdev_source.py  # Fetch and normalize models.dev data
  arena_source.py      # Load and normalize arena-catalog data
  mapper.py            # ID alignment and fused raw records
  builder.py           # Main build_catalog pipeline + tier derivation
  cache.py             # Catalog caching to disk (TTL-based)
  data/
    overrides.json     # Static mapping overrides for IDs


⸻

2. Spec Sheet per Module

2.1 schema.py

Responsibility
Define all data models used within Catalog, including raw source records and the canonical model representation.

Inputs
None (pure type definitions).

Outputs / Interfaces
	•	AnyLLMModel
	•	ModelsDevModel
	•	ArenaModel
	•	FusedRaw
	•	CanonicalModel
	•	Catalog

Key Details
	•	AnyLLMModel:
	•	provider: str (e.g. "openai")
	•	model_id: str (e.g. "gpt-4.1-mini")
	•	ModelsDevModel:
	•	Fields for capabilities, cost, limits, metadata as scraped from models.dev/api.json.
	•	ArenaModel:
	•	arena_id: str
	•	rating: float
	•	rating_q025: float | None
	•	rating_q975: float | None
	•	category: str (e.g. "overall_text")
	•	FusedRaw:
	•	canonical_id: str (e.g. "openai/gpt-4.1-mini")
	•	anyllm: AnyLLMModel
	•	modelsdev: ModelsDevModel | None
	•	arena: ArenaModel | None
	•	CanonicalModel (core business object):
	•	Basic:
	•	canonical_id, provider, model_id, family, display_name
	•	Capabilities:
	•	supports_reasoning, supports_tool_call, supports_structured_output
	•	input_modalities, output_modalities, attachments
	•	Limits:
	•	context_tokens, max_input_tokens, max_output_tokens
	•	Pricing:
	•	price_input_per_million, price_output_per_million, price_reasoning_per_million
	•	Derived tiers:
	•	quality_tier, reasoning_tier, creative_tier, cost_tier (all 1–5)
	•	Quality:
	•	arena_score, arena_ci_low, arena_ci_high
	•	Meta:
	•	knowledge_cutoff, release_date, last_updated, open_weights
	•	tags: list[str]
	•	Catalog:
	•	catalog_version: int
	•	built_at: str
	•	models: list[CanonicalModel]

⸻

2.2 anyllm_source.py

Responsibility
Discover all models that are actually callable via any-llm given local environment and config.

Inputs
	•	any-llm Python SDK / config.
	•	Environment with provider API keys.

Outputs / Interfaces
	•	Function:

def load_anyllm_models() -> list[AnyLLMModel]:
    ...



Logic
	•	Use any-llm’s configuration/introspection to list:
	•	providers available,
	•	models available per provider.
	•	For each, build an AnyLLMModel.
	•	Filter out models that are clearly unusable (e.g. disabled or missing mandatory config) if any-llm exposes such info.

⸻

2.3 modelsdev_source.py

Responsibility
Fetch and normalize model metadata from https://models.dev/api.json.

Inputs
	•	HTTP access to https://models.dev/api.json.

Outputs / Interfaces
	•	Functions:

def fetch_modelsdev_json() -> dict:
    """HTTP GET models.dev/api.json, return parsed dict."""

def normalize_modelsdev(data: dict) -> dict[str, ModelsDevModel]:
    """
    Flatten provider → models into dict keyed by a canonical-like ID
    e.g. "openai/gpt-4.1-mini".
    """



Logic
	•	fetch_modelsdev_json():
	•	Make a GET request.
	•	Basic retry & error handling.
	•	normalize_modelsdev():
	•	For each provider and each model under models:
	•	Build a ModelsDevModel.
	•	Normalize the ID into a canonical_id string, typically "provider/model_id".
	•	Return mapping: canonical_id -> ModelsDevModel.

⸻

2.4 arena_source.py

Responsibility
Load and normalize LMArena’s arena-catalog leaderboard data.

Inputs
	•	Path to JSON generated by update_leaderboard_data.py (provided via env or default location).
	•	JSON structure as defined by lmarena/arena-catalog.

Outputs / Interfaces
	•	Function:

def load_arena_models(path: Path | None = None) -> dict[str, ArenaModel]:
    """
    Load arena leaderboard JSON and normalize to dict keyed by arena_id.
    """



Logic
	•	Read JSON (e.g. leaderboard-text.json).
	•	Select a canonical category (for v1, a single “overall text” or similar).
	•	For each model row:
	•	Extract model name as arena_id.
	•	Extract rating, rating_q025, rating_q975.
	•	Build ArenaModel.
	•	Return dict[arena_id, ArenaModel].

⸻

2.5 mapper.py

Responsibility
Align IDs between any-llm, models.dev, and arena-catalog; produce fused raw records.

Inputs
	•	list[AnyLLMModel] from anyllm_source.
	•	dict[str, ModelsDevModel] from modelsdev_source.
	•	dict[str, ArenaModel] from arena_source.
	•	overrides.json in llmhub/catalog/data/.

Outputs / Interfaces
	•	Data model: FusedRaw.
	•	Function:

def load_overrides() -> dict[str, dict]:
    """Load static ID overrides from data/overrides.json."""

def fuse_sources(
    anyllm_models: list[AnyLLMModel],
    modelsdev_map: dict[str, ModelsDevModel],
    arena_map: dict[str, ArenaModel],
    overrides: dict[str, dict]
) -> list[FusedRaw]:
    ...



Logic
	•	canonical_id convention:
	•	canonical_id = f"{provider}/{model_id}" from AnyLLMModel.
	•	For each AnyLLMModel:
	•	Look up ModelsDevModel using canonical_id:
	•	direct match; if none, try any mapping rules from overrides.
	•	Look up ArenaModel:
	•	direct match: arena_map.get(canonical_id) or by stripped name.
	•	if overrides define arena_id for this canonical_id, use that.
	•	Build FusedRaw with:
	•	canonical_id
	•	anyllm (always present)
	•	modelsdev (optional)
	•	arena (optional)
	•	Return list of FusedRaw records.

⸻

2.6 builder.py

Responsibility
Coordinate the full build pipeline, compute global statistics, derive CanonicalModels, and produce Catalog.

Inputs
	•	anyllm_source, modelsdev_source, arena_source, mapper, cache.

Outputs / Interfaces
	•	Public entrypoint:

def build_catalog(
    ttl_hours: int = 24,
    force_refresh: bool = False
) -> Catalog:
    ...


	•	Internal helpers:

def _compute_global_stats(fused: list[FusedRaw]) -> GlobalStats:
    """
    Compute price and arena-score quantiles for tier bucketing.
    """

def _derive_canonical(
    f: FusedRaw,
    stats: GlobalStats
) -> CanonicalModel:
    ...



Logic (high-level)
	1.	Cache check
	•	Ask cache.load_cached_catalog(ttl_hours).
	•	If result present and force_refresh=False: return cached catalog.
	2.	Source loading
	•	anyllm_models = load_anyllm_models()
	•	modelsdev_data = fetch_modelsdev_json() + modelsdev_map = normalize_modelsdev(modelsdev_data)
	•	arena_map = load_arena_models(...)
	•	overrides = load_overrides()
	•	fused_raw = fuse_sources(anyllm_models, modelsdev_map, arena_map, overrides)
	3.	Global stats
	•	Compute quantiles for:
	•	price (e.g. average of input/output cost per million),
	•	arena_score (if available).
	•	Put into a GlobalStats struct used for tier mappings.
	4.	Derive canonical models
	•	For each FusedRaw:
	•	Build CanonicalModel:
	•	Basic identity and display name.
	•	Capabilities from models.dev flags (if present); sane defaults if missing.
	•	Limits from models.dev.
	•	Pricing from models.dev.
	•	arena_score from ArenaModel.
	•	Derived tiers:
	•	cost_tier from price quantiles.
	•	quality_tier from arena_score quantiles (fallback by provider/family).
	•	reasoning_tier:
	•	base on quality_tier, bump if modelsdev.reasoning=True.
	•	creative_tier:
	•	start = quality_tier; can later be refined.
	•	tags built from:
	•	reasoning, tools, structured_output, modalities, open_weights, etc.
	5.	Catalog object and cache
	•	Compose Catalog with:
	•	catalog_version = 1
	•	built_at = now()
	•	models = list[CanonicalModel]
	•	Call cache.save_catalog(catalog).
	•	Return catalog.

⸻

2.7 cache.py

Responsibility
Cache Catalog JSON to disk with TTL.

Inputs
	•	Catalog object.
	•	Configurable TTL (hours).

Outputs / Interfaces
	•	Functions:

def load_cached_catalog(ttl_hours: int) -> Catalog | None:
    ...

def save_catalog(cat: Catalog) -> None:
    ...



Logic
	•	load_cached_catalog():
	•	If file does not exist → return None.
	•	Check modification time vs now:
	•	If age > ttl_hours → return None.
	•	Else → parse JSON into Catalog and return.
	•	save_catalog():
	•	Serialize Catalog to JSON.
	•	Write to ~/.config/llmhub/catalog.json (create dirs if needed).

⸻

2.8 __init__.py

Responsibility
Provide a clean public API surface.

Interfaces
	•	Re-export:

from .schema import Catalog, CanonicalModel
from .builder import build_catalog
from .cache import load_cached_catalog



⸻

3. CLI Integration (Summary Spec)

In llmhub/cli.py (or existing CLI module):
	•	llmhub catalog show
	•	Calls build_catalog().
	•	Prints:
	•	number of models,
	•	per-provider summary,
	•	for each provider, a small table with:
	•	model_id, cost_tier, quality_tier, arena_score.
	•	llmhub catalog refresh
	•	Calls build_catalog(force_refresh=True).
	•	Prints summary (models count and timestamp).

⸻

4. Acceptance Criteria

Catalog v1 is considered complete when:
	1.	Running llmhub catalog refresh with at least one provider key set:
	•	Produces ~/.config/llmhub/catalog.json.
	•	JSON contains non-empty models array.
	•	Each CanonicalModel has:
	•	populated identity, capabilities, limits, and at least partial pricing.
	2.	Running llmhub catalog show:
	•	Uses cached catalog if fresh.
	•	Prints a readable summary without errors.
	3.	build_catalog():
	•	Returns a Catalog object when called from Python.
	•	Handles models missing from models.dev or arena-catalog gracefully (still present, with reduced fields and conservative tiers).
	4.	The generator can rely solely on:
	•	Catalog and CanonicalModel types, plus build_catalog(),
	•	without needing to know about models.dev, arena-catalog, or any-llm internals.
	5.	All I/O and external API calls are contained within:
	•	anyllm_source.py, modelsdev_source.py, arena_source.py, and cache.py.
	•	builder.py, mapper.py and schema.py remain pure business logic.

This plan should be sufficient for an IDE/agent (e.g. Antigravity) to implement Catalog v1 consistently.

---

### Prompt for Antigravity (concise)

```text
You are Antigravity, implementing a new Catalog feature for the existing `llm-hub` repo.

Context:
- The goal is to build a local model catalog using three inputs:
  1) any-llm (models I can actually call on this machine),
  2) models.dev (https://models.dev/api.json – hard metadata: pricing, limits, modalities, flags),
  3) LMArena arena-catalog (JSON generated by update_leaderboard_data.py – quality scores). (the file is added in the plan, read it and consume it)
- The Catalog’s job is:
  - Discover available models via any-llm.
  - Enrich them with metadata from models.dev and quality scores from arena-catalog.
  - Fuse all of this into a canonical per-model record with cost/quality/capability tiers.
  - Cache the result to disk and expose it to CLI and generator.

Task:
1. Open and read `PLAN-catalog-v1.md` in the repo root.
2. Implement everything described there under `packages/llmhub/src/llmhub/catalog/`:
   - `schema.py`, `anyllm_source.py`, `modelsdev_source.py`, `arena_source.py`,
     `mapper.py`, `builder.py`, `cache.py`, `__init__.py`, and `data/overrides.json`.
   - Respect the spec sheets for each module:
     - inputs, outputs, and responsibilities must match the plan.
     - `CanonicalModel` and `Catalog` must be stable types.
   - Make sure external I/O (HTTP, disk) is isolated to the source + cache modules.
3. Integrate Catalog into the CLI:
   - Add `llmhub catalog show` and `llmhub catalog refresh` commands.
   - `refresh` calls `build_catalog(force_refresh=True)` and prints a summary.
   - `show` calls `build_catalog()` and prints a compact per-provider model table.
4. Ensure:
   - If no providers are configured, commands fail gracefully with a clear message.
   - If models.dev or arena data is missing, Catalog still builds with whatever fields are available.
   - Cache TTL logic works as per the plan.

Do not change unrelated parts of the repo.
Follow existing coding style and packaging conventions.
When in doubt, defer to the behavior described in PLAN-catalog-v1.md.